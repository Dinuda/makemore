{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be51aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db70cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4280140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d33098bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            \n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a3362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into splits\n",
    "\n",
    "import random\n",
    "random.seed(21476891001)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3733c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP impl\n",
    "n_embd = 10 # dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # dimensionality of neurons in the hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(21476891001)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g) \n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "b2 = torch.randn((vocab_size), generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ba52502",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m emb \u001b[38;5;241m=\u001b[39m C[Xb]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39munbind(emb, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m     15\u001b[0m embcat \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mview(emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# optimization / training\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    # train with mini batches\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    print(torch.unbind(emb, 1).size)\n",
    "    embcat = emb.view(emb.shape[0], -1) # = \n",
    "#     print(embcat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
